# Virtual Adversarial Training:A Regularization Method for Supervised and Semi-Supervised Learning

虚拟对抗训练：对于监督和半监督学习的正则化方法

此作者为Takeru Miyato发表于2018IEEE.

## 摘要

提出了一个基于虚拟对抗损失的新的正则化方法：给定输入的条件标签分布的局部平滑度的新度量。虚拟对抗性损失被定义为围绕每个输入数据点的条件标签分布对局部扰动的鲁棒性。与对抗训练不同，作者的方法没有根据标签的信息定义对抗方向，因此适用于半监督学习。因为我们平滑模型的方向只是“虚拟”对抗，我们称之为虚拟对抗训练方法（VAT）。VAT的计算成本相对较低。对于神经网络，可以使用不超过两对前向和后向传播来计算虚拟对抗损失的近似梯度。在我们的实验中，我们将VAT应用于多个基准数据集上的监督和半监督学习任务。通过基于熵最小化原理的算法的简单增强，我们的VAT在SVHN和CIFAR-10上实现了半监督学习任务的最先进性能。

## 1.介绍

在实际的回归和分类问题中，必须面对两个问题; 欠拟合和过拟合。一方面，模型和优化过程的不良设计可能导致训练和测试数据集的大误差（欠拟合）。另一方面，可用于调整模型参数的样本的大小总是有限的，在实践中对目标函数的评估总是仅仅是对样本空间上目标值的真实期望的经验近似。因此，即使在训练数据集上成功优化且有较低的错误率（训练错误），真正的预期误差（测试误差）也可能很大（过度拟合）。作者研究的就是过拟合的问题。正规化是引入附加信息的过程，以便管理训练误差和测试误差之间不可避免的差距。在本研究中，我们引入了一种适用于半监督学习的新型正则化方法，该方法确定了分类器行为最敏感的方向。正则化通常通过使用所谓的正则项来增加损失函数来执行，该正则化项防止模型过度拟合到在有限的样本点集上评估的损失函数。从贝叶斯观点来看，正则化项可以被解释为先验分布，反映了我们受过教育的关于模型的先验知识或信念。基于广泛观察到的事实，先验信念是大多数自然发生的系统的输出在空间和/或时间输入方面是平滑的。这一信念的基础往往是控制兴趣系统的物理定律，在许多情况下，这些定律是由基于微分方程的光滑模型描述的。当我们构建概率模型时，这个信念促使我们更喜欢条件输出分布p(y|x)(或者简称为输出分布)，它相对于条件输入x是平滑的。

实际上，平滑输出分布通常对我们在实际操作中的优势起作用。 例如，标签传播[49]是一种算法，它通过将类标签分配给未标记的训练样本来改进分类器的性能，这是基于相信输入数据点倾向于具有相似类标签的信念。此外，对于神经网络(NNs)，可以通过对每个输入施加随机扰动来生成人工输入点，并鼓励模型将相似的输出分配给同一点的人工输入集，从而提高泛化性能。一些研究也证实，使预测器对随机和局部扰动具有鲁棒性的这种哲学在半监督学习中是有效的。但是这种哲学有应用弱点。他们发现通过随机噪声和随机数据增强的标准各向同性平滑常常使预测器特别容易受到特定方向上的小扰动，即对抗方向。所谓对抗方向即输入空间中模型的标签概率p（y =k|x）最敏感的方向.Goodfellow通过实验证实，当信号在对抗方向受到扰动时，使用标准正则化技术（如L1和L2正则化）训练的预测变量可能会出错，即使扰动的范数很小， 人眼无法察觉它。

虚拟对抗方向的概念，它是扰动的方向，可以最大程度地改变分布发散意义上的输出分布。对抗方向是在输入数据点处是扰动的方向，其可以最大程度地降低模型的正确分类概率，或者可以最大程度地“偏离”模型的预测与正确标签的方向。与对抗方向不同，虚拟对抗方向可以在未标记的数据点上定义，因为它是可以最大程度地偏离当前推断的输出分布与现状的方向。 换句话说，即使没有标签信息，也可以在未标记的数据点上定义虚拟对抗方向，就好像存在“虚拟”标签一样; 因此名称为“虚拟”对抗方向。

通过虚拟对抗方向的定义，我们可以在不使用监控信号的情况下量化每个输入点处模型的局部各向异性。 我们将局部分布平滑度（LDS）定义为模型对虚拟对抗方向的基于分歧的分布鲁棒性。我们提出了一种新的训练方法，该方法使用有效的近似来最大限度地提高模型的似然，同时提高模型在每个训练输入数据点上的LDS。为了简单起见，我们将此方法称为虚拟对抗训练(VAT)。以下列表总结了这种新方法的优点：

+ 适用于半监督学习任务
+ 适用于任何参数模型，我们可以根据输入和参数评估梯度
+ 少数超参数
+ 参数化不变正则化

